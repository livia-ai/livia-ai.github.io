<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<meta name="description" content="" />
		<link rel="icon" href="../favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta http-equiv="content-security-policy" content="">
	<link rel="stylesheet" href="/_app/assets/app-b3365922.css">
	<link rel="stylesheet" href="/_app/assets/pages/blog/__layout.svelte-27041123.css">
	<link rel="modulepreload" href="/_app/start-fa70173e.js">
	<link rel="modulepreload" href="/_app/chunks/vendor-ea2447eb.js">
	<link rel="modulepreload" href="/_app/pages/__layout.svelte-930164ba.js">
	<link rel="modulepreload" href="/_app/pages/blog/__layout.svelte-1d72bd6a.js">
	<link rel="modulepreload" href="/_app/pages/blog/2022-04-machines-reading-metadata.md-7a356552.js"><script type="module">
		import { start } from "/_app/start-fa70173e.js";
		start({
			target: document.querySelector("body"),
			paths: {"base":"","assets":""},
			session: {},
			route: true,
			spa: false,
			trailing_slash: "never",
			hydrate: {
				status: 200,
				error: null,
				nodes: [
					import("/_app/pages/__layout.svelte-930164ba.js"),
						import("/_app/pages/blog/__layout.svelte-1d72bd6a.js"),
						import("/_app/pages/blog/2022-04-machines-reading-metadata.md-7a356552.js")
				],
				url: new URL("http://sveltekit-prerender/blog/2022-04-machines-reading-metadata"),
				params: {}
			}
		});
	</script>
	</head>
	<body class="leading-relaxed tracking-normal text-white gradient" style="font-family: 'Source Sans Pro', sans-serif;">		
		


<nav id="header" class="fixed w-full z-30 top-0 text-white shadow bg-white"><div class="w-full container mx-auto flex flex-wrap items-center justify-between mt-0 py-2"><div class="pl-4 flex items-center"><a class="text-gray-800 no-underline hover:no-underline font-bold text-2xl lg:text-4xl" href="/">LiviaAI
      </a></div>
    <div class="w-full flex-grow lg:flex lg:items-center lg:w-auto hidden mt-2 lg:mt-0 bg-white lg:bg-transparent text-black p-4 lg:p-0 z-20" id="nav-content"><ul class="list-reset lg:flex justify-end flex-1 items-center"><li class="mr-3"><a class="inline-block text-black no-underline hover:text-gray-800 hover:text-underline py-2 px-4" href="/">Home</a></li>
        <li class="mr-3"><a class="inline-block py-2 px-4 text-black font-bold no-underline" href="/blog/sample">Blog</a></li></ul></div></div>
  <hr class="border-b border-gray-100 opacity-25 my-0 py-0"></nav>

<section class="bg-white py-8"><div class="container max-w-5xl mx-auto m-8 blogpost svelte-1rn5i04"><div class="w-6/6 sm:w-2/2 p-6 text-gray-600"><h1>Machines Reading Metadata</h1>
<p>We are now six weeks into our project, and it’s time for an update on the work of our tech team! The tech team, by the way, that’s us: intern and Machine Learning student <a href="#">Bernhard</a>, and me, <a href="#">Rainer</a>, Senior Researcher at the Data Science &amp; Artificial Intelligence group at the Austrian Institute of Technology.</p>
<p>As we explained in our <a href="#">introductory post</a>, the main goal of LiviaAI is to find out whether we can teach computers to judge “similarity” of artworks. Why?
For two reasons: first, this way museums in the future will be able to <strong>more easily connect their collections to those of other museums</strong>. Computers would handle most of the tedious, time-consuming work of sorting through thousands of artworks, and finding relevant related material in other museums, thus saving museum curators many hours of their valuable time. Second, we believe that these new connections will enable <strong>new forms of online museum experiences that cross institutional boundaries</strong>. Interested in the <a href="https://en.wikipedia.org/wiki/Wiener_Moderne" rel="nofollow">Wiener Moderne</a>? Then why stay within the confines of just one or two famous museums, when other, smaller, museums might house some lesser known treasures, too?</p>
<h2>Similar or Not?</h2>
<p>The big question is, of course: what <em>does</em> make two artworks similar? And how do we even get enough of the right data to teach a machine to mimic our understanding of similarity? Previous approaches have been looking, for example, at similar colours. Or they used artificial intelligence to detect objects in the artworks. But would you necessarily consider two artworks similar just because they both use yellow as their primary color? Or because they both depict flowers?</p>
<p>In LiviaAI, we want to take a different approach. Instead of coming up with our own, technical, definitions of similarity, or relying on simple metrics, we want to leverage the existing knowledge that museum curators have accumulated in their collections over the years. Before teaching computers how to recognize similarity in the <em>images</em>, we first want to understand how curators have been describing their artworks, and search for similarities in their <em>descriptions</em>. </p>
<h2>Understanding our Partner Collections</h2>
<p>The first step in our journey is therefore to get a better understanding of our partner collections. What’s actually inside them? What information have curators added to the items as metadata over the years? And how can we translate all this existing knowledge - and hard work - into data that computers can work with?</p>
<p>[…unfinished… material for re-use …]</p>
<ul><li>First, we will collect examples of images that are “similar” in some way. We will also collect counter-examples, so that the
computer can learn what “different” images look like. (More on the ways in which images might resemble or differ later.) </li>
<li>Second, we will use the examples to train an AI algorithm, so that it learns how “similarity” manifests itself in the image. Or,
in other words: so that it learns what to look for in similar vs. different images. (This process is called
<a href="https://en.wikipedia.org/wiki/Feature_learning" rel="nofollow">representation or feature learning</a>).</li></ul>
<p>The bad news: we need <strong>a lot</strong> of examples to train the AI. We expect that we’ll need at least 10.000 examples as the absolute minimum. And that’s a much bigger training set than we could ever assemble by hand.</p>
<iframe src="/embeds/blog/2022-04/embeddings-example.html" style="width:800px; height:600px;"></iframe></div></div></section>

<svg class="wave-top" viewBox="0 0 1439 147" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g transform="translate(-1.000000, -14.000000)" fill-rule="nonzero"><g class="wave" fill="#fff"><path d="M1440,84 C1383.555,64.3 1342.555,51.3 1317,45 C1259.5,30.824 1206.707,25.526 1169,22 C1129.711,18.326 1044.426,18.475 980,22 C954.25,23.409 922.25,26.742 884,32 C845.122,37.787 818.455,42.121 804,45 C776.833,50.41 728.136,61.77 713,65 C660.023,76.309 621.544,87.729 584,94 C517.525,105.104 484.525,106.438 429,108 C379.49,106.484 342.823,104.484 319,102 C278.571,97.783 231.737,88.736 205,84 C154.629,75.076 86.296,57.743 0,32 L0,0 L1440,0 L1440,84 Z"></path></g><g transform="translate(1.000000, 15.000000)" fill="#FFFFFF"><g transform="translate(719.500000, 68.500000) rotate(-180.000000) translate(-719.500000, -68.500000) "><path d="M0,0 C90.7283404,0.927527913 147.912752,27.187927 291.910178,59.9119003 C387.908462,81.7278826 543.605069,89.334785 759,82.7326078 C469.336065,156.254352 216.336065,153.6679 0,74.9732496" opacity="0.100000001"></path><path d="M100,104.708498 C277.413333,72.2345949 426.147877,52.5246657 546.203633,45.5787101 C666.259389,38.6327546 810.524845,41.7979068 979,55.0741668 C931.069965,56.122511 810.303266,74.8455141 616.699903,111.243176 C423.096539,147.640838 250.863238,145.462612 100,104.708498 Z" opacity="0.100000001"></path><path d="M1046,51.6521276 C1130.83045,29.328812 1279.08318,17.607883 1439,40.1656806 L1439,120 C1271.17211,77.9435312 1140.17211,55.1609071 1046,51.6521276 Z" opacity="0.200000003"></path></g></g></g></g></svg>


	</body>
</html>
