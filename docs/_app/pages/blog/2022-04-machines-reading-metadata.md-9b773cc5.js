import{S as Wa,i as Ma,s as ja,C as Gt,w as Ra,x as Oa,y as za,z as La,A as Ta,q as Da,o as Fa,B as qa,Q as Ia,e as s,t as o,k as d,c as n,a as l,h as r,d as a,m as u,b as c,O as Ha,f as Sa,g as h,H as t}from"../../chunks/vendor-f435face.js";import{P as $a}from"../../chunks/_post-4f5578a2.js";function Ca(K){let f,b,y,M,m,p,V,Fe,me,w,qe,Q,He,$e,ne,Ce,Ne,le,Ge,Ue,j,Ke,Ve,pe,J,Qe,we,T,Je,he,Xe,Ye,ge,_,Ze,ce,et,tt,fe,at,ot,ve,I,rt,R,it,st,ye,X,nt,be,Y,lt,_e,v,ht,O,ct,ft,z,dt,ut,L,mt,pt,ke,Z,wt,Ee,ee,gt,xe,k,vt,D,yt,bt,de,_t,kt,Ae,g,Et,F,xt,At,q,Tt,It,H,St,Pt,$,Bt,Wt,Te,te,Mt,Ie,S,jt,C,Rt,Ot,Se,A,Ut,Pe,N,zt,Be,P,Lt,G,Dt,Ft,We,ae,qt,Me,U,oe,Ht,je,B,ue,$t,Ct,re,Nt;return{c(){f=s("p"),b=o("It\u2019s time for the first update from the tech team! The tech team, that\u2019s us: intern "),y=s("strong"),M=o("Bernhard"),m=o(", student of Artificial Intelligence at the University of Linz, and me, "),p=s("a"),V=o("Rainer"),Fe=o(", Senior Researcher at the Data Science & Artificial Intelligence group at the Austrian Institute of Technology."),me=d(),w=s("p"),qe=o("As we explained in our "),Q=s("a"),He=o("introductory post"),$e=o(", one of the goals of LiviaAI is to find out if we can teach computers to recognize \u201Csimilarity\u201D of artworks. Why? For two reasons: first, because this will allow museums in the future to "),ne=s("strong"),Ce=o("connect their collections to those of other museums more easily"),Ne=o(". Computers can handle most of the tedious, time-consuming work of sorting through thousands of artworks, and finding relevant related material in other museums, thus saving museum curators many hours of their valuable time. Second, we believe that these new connections will enable "),le=s("strong"),Ge=o("new forms of online museum experiences that cross institutional boundaries"),Ue=o(". Interested in the "),j=s("a"),Ke=o("Wiener Moderne"),Ve=o("? Then why stay within the confines of just one or two famous museums, when other, smaller, museums might house some lesser known treasures, too?"),pe=d(),J=s("h2"),Qe=o("Similar or Not?"),we=d(),T=s("p"),Je=o("The big question is, of course: what "),he=s("em"),Xe=o("does"),Ye=o(" make two artworks similar? And how do we even get enough of the right data to teach a machine to mimic our understanding of similarity? Previous approaches have been looking, for example, at similar colours. Or they used artificial intelligence to detect common objects in the artworks. But would you necessarily consider two artworks similar just because they both use yellow as their primary color? Or because they both depict flowers?"),ge=d(),_=s("p"),Ze=o("In LiviaAI, we want to take a different approach. Instead of coming up with our own metrics for similarity, or relying on technical properties like color, we want to leverage the existing knowledge that museum curators have accumulated in their collections over many years. In order to teach computers how to recognize similar "),ce=s("em"),et=o("images"),tt=o(", we first want to understand how curators have been describing them - and search for similarity in these "),fe=s("em"),at=o("descriptions"),ot=o("."),ve=d(),I=s("p"),rt=o("Once we have identified artworks that are described in similar terms, we will use the images as examples to train the algorithm. This is also called "),R=s("a"),it=o("representation or feature learning"),st=o(", because the algorithm learns - by itself - how the representation of similiarity manifests itself in the images."),ye=d(),X=s("h2"),nt=o("Understanding Our Partner Collections"),be=d(),Y=s("p"),lt=o("The first step in our journey is therefore to gain an understanding of our partner collections. What\u2019s inside them? What information have curators added to the items as metadata? And how can we translate all this existing knowledge (and years of work!) into data that can teach computers to recognize similar art?"),_e=d(),v=s("p"),ht=o("Our three partners, the "),O=s("a"),ct=o("Belvedere"),ft=o(", the "),z=s("a"),dt=o("MAK"),ut=o(" and the "),L=s("a"),mt=o("Wien Museum"),pt=o(" have given us access to metadata for all artworks they display in their public online collections. In total, that\u2019s more than 300.000 objects! The metadata includes a mix of information such as: title and description text, subjects keywords and classification terms, artist names, summary information on provenance or exhibition history, information about material, production, places and time periods relevant to the artwork, etc."),ke=d(),Z=s("p"),wt=o("Each partner uses different documentation practices, and has different priorities when describing their artworks. Properties that are recorded in the MAK collection, for example, may not exist (or be relevant!) in the Belvedere collection. Therefore, figuring out which artworks might be similar - according to what the metadata says - is not as straightforward as one might hope."),Ee=d(),ee=s("h2"),gt=o("Reading Metadata - From a Distance"),xe=d(),k=s("p"),vt=o("Luckily, AI can help us here, too. In LiviaAI, we use a technique called "),D=s("a"),yt=o("Sentence Embedding"),bt=o(" to get a better picture of the structure, themes and topics of our collections. Sentence Embedding is a computational method that transforms written text - such as a sentence or a paragraph - to a "),de=s("em"),_t=o("vector"),kt=o(", a numeric representation that we can treat as a point in space. Sentences that share similar semantics will be located nearby in that space, which means we can deduct information about the similarity of two texts simply by measuring the distance between them."),Ae=d(),g=s("p"),Et=o("There are different ways to compute embeddings. Some algorithms (like "),F=s("a"),xt=o("LSI"),At=o(" or "),q=s("a"),Tt=o("Doc2Vec"),It=o(") start from a blank state, with no prior knowledge of language or vocabulary. They take the full dataset as input (say, the fulltext of a novel), and fit the sentence representations so that sentences that use (or are surrounded by) similar vocabulary will end up close together. Other approaches start from pre-trained models, which means they will already come pre-loaded with knowledge trained from a large text corpus. In our case, we use "),H=s("a"),St=o("BERT"),Pt=o(" a large language model developed in 2018 by Google researchers, which was shortly after adapted for the purpose of high-performance "),$=s("a"),Bt=o("sentence embedding"),Wt=o("."),Te=d(),te=s("p"),Mt=o("The advantage of a pre-trained model is that it already encodes a lot of language context. In simplified terms, BERT can already tell you that a \u201Cpair of boots\u201D is closer in meaning to a \u201Cpair of sneakers\u201D than a \u201Cpair of pants\u201D. The terms for \u201Cpencil drawing\u201D would have a certain distance from \u201Cphotograph\u201D. But the distance between a \u201Cpencil drawing of a person\u201D and a \u201Cphotograph of a person\u201D would be smaller."),Ie=d(),S=s("p"),jt=o("Here\u2019s an example for how BERT grants us a new perspective on our data. The diagram below visualizes 6.200 records from the Wien Museum. (That\u2019s only about 10% of their total collection. But it helps to make the diagram load a bit faster in this blogpost.) Each metadata record from the collection is represented as a point in 3D space"),C=s("a"),Rt=o("1"),Ot=o(". We have colored the dots according to the categories that curators have assigned them in the collection management system. This means we\u2019d expect a to see a pattern where dots of similar color should form clusters. And - luckily - that\u2019s the case. The diagram is interactive! Try zooming/panning with mouse or touch, and explore the clusters that have emerged."),Se=d(),A=s("iframe"),Pe=d(),N=s("span"),zt=o("Sentence embeddings for 6,200 metadata records from the online collection of the Wien Museum. Color represents a distinct combination of classifications assigned by the curators."),Be=d(),P=s("p"),Lt=o("We are currently sifting through our data, and are starting to draw samples of \u201Csimilar\u201D and \u201Cdifferent\u201D images to get a better grip on the possiblities and limitations of the approach. Picking training data based on sentence embeddings is an idea that was initially developed - and tested successfully - "),G=s("a"),Dt=o("in a different context by AIT colleague Alexander Schindler et al."),Ft=o(", who also inspired and advised on the preparation of this project. Which means we are\u2026 sort of\u2026 confident that this will work. But it\u2019s all crucially dependent on the data - quality, level of detail, whether curators have used free text vs. keywords, whether individual records differ sufficiently from each other, or whether information is often sparse, formulaic or repetitive, etc."),We=d(),ae=s("p"),qt=o("For now, the goal is only to prepare enough material for discussion at our upcoming workshop. But our journey is guaranteed to remain exciting!"),Me=d(),U=s("ol"),oe=s("li"),Ht=o("For the sake of completeness: sentence embeddings work in higher dimensional space. In our case, for example, we compute clusters and distances in 50 dimensions. I.e. technically, what you see in the diagram are 50-dimensional embeddings projected to three dimensions for the purpose of visualization. This also means that the 3D visualization is only an approximation and isn't always able to capture the true geometry of the cluster patterns."),je=d(),B=s("footer"),ue=s("h4"),$t=o("Previous post"),Ct=d(),re=s("a"),Nt=o("Hello World!"),this.h()},l(e){f=n(e,"P",{});var i=l(f);b=r(i,"It\u2019s time for the first update from the tech team! The tech team, that\u2019s us: intern "),y=n(i,"STRONG",{});var Kt=l(y);M=r(Kt,"Bernhard"),Kt.forEach(a),m=r(i,", student of Artificial Intelligence at the University of Linz, and me, "),p=n(i,"A",{href:!0,rel:!0});var Vt=l(p);V=r(Vt,"Rainer"),Vt.forEach(a),Fe=r(i,", Senior Researcher at the Data Science & Artificial Intelligence group at the Austrian Institute of Technology."),i.forEach(a),me=u(e),w=n(e,"P",{});var E=l(w);qe=r(E,"As we explained in our "),Q=n(E,"A",{href:!0});var Qt=l(Q);He=r(Qt,"introductory post"),Qt.forEach(a),$e=r(E,", one of the goals of LiviaAI is to find out if we can teach computers to recognize \u201Csimilarity\u201D of artworks. Why? For two reasons: first, because this will allow museums in the future to "),ne=n(E,"STRONG",{});var Jt=l(ne);Ce=r(Jt,"connect their collections to those of other museums more easily"),Jt.forEach(a),Ne=r(E,". Computers can handle most of the tedious, time-consuming work of sorting through thousands of artworks, and finding relevant related material in other museums, thus saving museum curators many hours of their valuable time. Second, we believe that these new connections will enable "),le=n(E,"STRONG",{});var Xt=l(le);Ge=r(Xt,"new forms of online museum experiences that cross institutional boundaries"),Xt.forEach(a),Ue=r(E,". Interested in the "),j=n(E,"A",{href:!0,rel:!0});var Yt=l(j);Ke=r(Yt,"Wiener Moderne"),Yt.forEach(a),Ve=r(E,"? Then why stay within the confines of just one or two famous museums, when other, smaller, museums might house some lesser known treasures, too?"),E.forEach(a),pe=u(e),J=n(e,"H2",{});var Zt=l(J);Qe=r(Zt,"Similar or Not?"),Zt.forEach(a),we=u(e),T=n(e,"P",{});var Re=l(T);Je=r(Re,"The big question is, of course: what "),he=n(Re,"EM",{});var ea=l(he);Xe=r(ea,"does"),ea.forEach(a),Ye=r(Re," make two artworks similar? And how do we even get enough of the right data to teach a machine to mimic our understanding of similarity? Previous approaches have been looking, for example, at similar colours. Or they used artificial intelligence to detect common objects in the artworks. But would you necessarily consider two artworks similar just because they both use yellow as their primary color? Or because they both depict flowers?"),Re.forEach(a),ge=u(e),_=n(e,"P",{});var ie=l(_);Ze=r(ie,"In LiviaAI, we want to take a different approach. Instead of coming up with our own metrics for similarity, or relying on technical properties like color, we want to leverage the existing knowledge that museum curators have accumulated in their collections over many years. In order to teach computers how to recognize similar "),ce=n(ie,"EM",{});var ta=l(ce);et=r(ta,"images"),ta.forEach(a),tt=r(ie,", we first want to understand how curators have been describing them - and search for similarity in these "),fe=n(ie,"EM",{});var aa=l(fe);at=r(aa,"descriptions"),aa.forEach(a),ot=r(ie,"."),ie.forEach(a),ve=u(e),I=n(e,"P",{});var Oe=l(I);rt=r(Oe,"Once we have identified artworks that are described in similar terms, we will use the images as examples to train the algorithm. This is also called "),R=n(Oe,"A",{href:!0,rel:!0});var oa=l(R);it=r(oa,"representation or feature learning"),oa.forEach(a),st=r(Oe,", because the algorithm learns - by itself - how the representation of similiarity manifests itself in the images."),Oe.forEach(a),ye=u(e),X=n(e,"H2",{});var ra=l(X);nt=r(ra,"Understanding Our Partner Collections"),ra.forEach(a),be=u(e),Y=n(e,"P",{});var ia=l(Y);lt=r(ia,"The first step in our journey is therefore to gain an understanding of our partner collections. What\u2019s inside them? What information have curators added to the items as metadata? And how can we translate all this existing knowledge (and years of work!) into data that can teach computers to recognize similar art?"),ia.forEach(a),_e=u(e),v=n(e,"P",{});var W=l(v);ht=r(W,"Our three partners, the "),O=n(W,"A",{href:!0,rel:!0});var sa=l(O);ct=r(sa,"Belvedere"),sa.forEach(a),ft=r(W,", the "),z=n(W,"A",{href:!0,rel:!0});var na=l(z);dt=r(na,"MAK"),na.forEach(a),ut=r(W," and the "),L=n(W,"A",{href:!0,rel:!0});var la=l(L);mt=r(la,"Wien Museum"),la.forEach(a),pt=r(W," have given us access to metadata for all artworks they display in their public online collections. In total, that\u2019s more than 300.000 objects! The metadata includes a mix of information such as: title and description text, subjects keywords and classification terms, artist names, summary information on provenance or exhibition history, information about material, production, places and time periods relevant to the artwork, etc."),W.forEach(a),ke=u(e),Z=n(e,"P",{});var ha=l(Z);wt=r(ha,"Each partner uses different documentation practices, and has different priorities when describing their artworks. Properties that are recorded in the MAK collection, for example, may not exist (or be relevant!) in the Belvedere collection. Therefore, figuring out which artworks might be similar - according to what the metadata says - is not as straightforward as one might hope."),ha.forEach(a),Ee=u(e),ee=n(e,"H2",{});var ca=l(ee);gt=r(ca,"Reading Metadata - From a Distance"),ca.forEach(a),xe=u(e),k=n(e,"P",{});var se=l(k);vt=r(se,"Luckily, AI can help us here, too. In LiviaAI, we use a technique called "),D=n(se,"A",{href:!0,rel:!0});var fa=l(D);yt=r(fa,"Sentence Embedding"),fa.forEach(a),bt=r(se," to get a better picture of the structure, themes and topics of our collections. Sentence Embedding is a computational method that transforms written text - such as a sentence or a paragraph - to a "),de=n(se,"EM",{});var da=l(de);_t=r(da,"vector"),da.forEach(a),kt=r(se,", a numeric representation that we can treat as a point in space. Sentences that share similar semantics will be located nearby in that space, which means we can deduct information about the similarity of two texts simply by measuring the distance between them."),se.forEach(a),Ae=u(e),g=n(e,"P",{});var x=l(g);Et=r(x,"There are different ways to compute embeddings. Some algorithms (like "),F=n(x,"A",{href:!0,rel:!0});var ua=l(F);xt=r(ua,"LSI"),ua.forEach(a),At=r(x," or "),q=n(x,"A",{href:!0,rel:!0});var ma=l(q);Tt=r(ma,"Doc2Vec"),ma.forEach(a),It=r(x,") start from a blank state, with no prior knowledge of language or vocabulary. They take the full dataset as input (say, the fulltext of a novel), and fit the sentence representations so that sentences that use (or are surrounded by) similar vocabulary will end up close together. Other approaches start from pre-trained models, which means they will already come pre-loaded with knowledge trained from a large text corpus. In our case, we use "),H=n(x,"A",{href:!0,rel:!0});var pa=l(H);St=r(pa,"BERT"),pa.forEach(a),Pt=r(x," a large language model developed in 2018 by Google researchers, which was shortly after adapted for the purpose of high-performance "),$=n(x,"A",{href:!0,rel:!0});var wa=l($);Bt=r(wa,"sentence embedding"),wa.forEach(a),Wt=r(x,"."),x.forEach(a),Te=u(e),te=n(e,"P",{});var ga=l(te);Mt=r(ga,"The advantage of a pre-trained model is that it already encodes a lot of language context. In simplified terms, BERT can already tell you that a \u201Cpair of boots\u201D is closer in meaning to a \u201Cpair of sneakers\u201D than a \u201Cpair of pants\u201D. The terms for \u201Cpencil drawing\u201D would have a certain distance from \u201Cphotograph\u201D. But the distance between a \u201Cpencil drawing of a person\u201D and a \u201Cphotograph of a person\u201D would be smaller."),ga.forEach(a),Ie=u(e),S=n(e,"P",{});var ze=l(S);jt=r(ze,"Here\u2019s an example for how BERT grants us a new perspective on our data. The diagram below visualizes 6.200 records from the Wien Museum. (That\u2019s only about 10% of their total collection. But it helps to make the diagram load a bit faster in this blogpost.) Each metadata record from the collection is represented as a point in 3D space"),C=n(ze,"A",{class:!0,href:!0});var va=l(C);Rt=r(va,"1"),va.forEach(a),Ot=r(ze,". We have colored the dots according to the categories that curators have assigned them in the collection management system. This means we\u2019d expect a to see a pattern where dots of similar color should form clusters. And - luckily - that\u2019s the case. The diagram is interactive! Try zooming/panning with mouse or touch, and explore the clusters that have emerged."),ze.forEach(a),Se=u(e),A=n(e,"IFRAME",{src:!0,style:!0});var Ba=l(A);Ba.forEach(a),Pe=u(e),N=n(e,"SPAN",{class:!0});var ya=l(N);zt=r(ya,"Sentence embeddings for 6,200 metadata records from the online collection of the Wien Museum. Color represents a distinct combination of classifications assigned by the curators."),ya.forEach(a),Be=u(e),P=n(e,"P",{});var Le=l(P);Lt=r(Le,"We are currently sifting through our data, and are starting to draw samples of \u201Csimilar\u201D and \u201Cdifferent\u201D images to get a better grip on the possiblities and limitations of the approach. Picking training data based on sentence embeddings is an idea that was initially developed - and tested successfully - "),G=n(Le,"A",{href:!0,rel:!0});var ba=l(G);Dt=r(ba,"in a different context by AIT colleague Alexander Schindler et al."),ba.forEach(a),Ft=r(Le,", who also inspired and advised on the preparation of this project. Which means we are\u2026 sort of\u2026 confident that this will work. But it\u2019s all crucially dependent on the data - quality, level of detail, whether curators have used free text vs. keywords, whether individual records differ sufficiently from each other, or whether information is often sparse, formulaic or repetitive, etc."),Le.forEach(a),We=u(e),ae=n(e,"P",{});var _a=l(ae);qt=r(_a,"For now, the goal is only to prepare enough material for discussion at our upcoming workshop. But our journey is guaranteed to remain exciting!"),_a.forEach(a),Me=u(e),U=n(e,"OL",{class:!0});var ka=l(U);oe=n(ka,"LI",{id:!0});var Ea=l(oe);Ht=r(Ea,"For the sake of completeness: sentence embeddings work in higher dimensional space. In our case, for example, we compute clusters and distances in 50 dimensions. I.e. technically, what you see in the diagram are 50-dimensional embeddings projected to three dimensions for the purpose of visualization. This also means that the 3D visualization is only an approximation and isn't always able to capture the true geometry of the cluster patterns."),Ea.forEach(a),ka.forEach(a),je=u(e),B=n(e,"FOOTER",{});var De=l(B);ue=n(De,"H4",{});var xa=l(ue);$t=r(xa,"Previous post"),xa.forEach(a),Ct=u(De),re=n(De,"A",{href:!0});var Aa=l(re);Nt=r(Aa,"Hello World!"),Aa.forEach(a),De.forEach(a),this.h()},h(){c(p,"href","https://rsimon.github.io"),c(p,"rel","nofollow"),c(Q,"href","/blog/2022-04-hello-world"),c(j,"href","https://en.wikipedia.org/wiki/Wiener_Moderne"),c(j,"rel","nofollow"),c(R,"href","https://en.wikipedia.org/wiki/Feature_learning"),c(R,"rel","nofollow"),c(O,"href","https://www.belvedere.at/"),c(O,"rel","nofollow"),c(z,"href","https://www.mak.at"),c(z,"rel","nofollow"),c(L,"href","https://www.wienmuseum.at/"),c(L,"rel","nofollow"),c(D,"href","https://en.wikipedia.org/wiki/Sentence_embedding"),c(D,"rel","nofollow"),c(F,"href","https://en.wikipedia.org/wiki/Latent_semantic_analysis"),c(F,"rel","nofollow"),c(q,"href","https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e"),c(q,"rel","nofollow"),c(H,"href","https://en.wikipedia.org/wiki/BERT_(language_model)"),c(H,"rel","nofollow"),c($,"href","https://arxiv.org/abs/1908.10084"),c($,"rel","nofollow"),c(C,"class","footnote"),c(C,"href","#footnote-1"),Ha(A.src,Ut="/embeds/blog/2022-04/embeddings-example.html")||c(A,"src",Ut),Sa(A,"width","800px"),Sa(A,"height","600px"),c(N,"class","image-caption centered"),c(G,"href","https://arxiv.org/pdf/2003.12265.pdf"),c(G,"rel","nofollow"),c(oe,"id","footnote-1"),c(U,"class","footnotes"),c(re,"href","/blog/2022-04-hello-world")},m(e,i){h(e,f,i),t(f,b),t(f,y),t(y,M),t(f,m),t(f,p),t(p,V),t(f,Fe),h(e,me,i),h(e,w,i),t(w,qe),t(w,Q),t(Q,He),t(w,$e),t(w,ne),t(ne,Ce),t(w,Ne),t(w,le),t(le,Ge),t(w,Ue),t(w,j),t(j,Ke),t(w,Ve),h(e,pe,i),h(e,J,i),t(J,Qe),h(e,we,i),h(e,T,i),t(T,Je),t(T,he),t(he,Xe),t(T,Ye),h(e,ge,i),h(e,_,i),t(_,Ze),t(_,ce),t(ce,et),t(_,tt),t(_,fe),t(fe,at),t(_,ot),h(e,ve,i),h(e,I,i),t(I,rt),t(I,R),t(R,it),t(I,st),h(e,ye,i),h(e,X,i),t(X,nt),h(e,be,i),h(e,Y,i),t(Y,lt),h(e,_e,i),h(e,v,i),t(v,ht),t(v,O),t(O,ct),t(v,ft),t(v,z),t(z,dt),t(v,ut),t(v,L),t(L,mt),t(v,pt),h(e,ke,i),h(e,Z,i),t(Z,wt),h(e,Ee,i),h(e,ee,i),t(ee,gt),h(e,xe,i),h(e,k,i),t(k,vt),t(k,D),t(D,yt),t(k,bt),t(k,de),t(de,_t),t(k,kt),h(e,Ae,i),h(e,g,i),t(g,Et),t(g,F),t(F,xt),t(g,At),t(g,q),t(q,Tt),t(g,It),t(g,H),t(H,St),t(g,Pt),t(g,$),t($,Bt),t(g,Wt),h(e,Te,i),h(e,te,i),t(te,Mt),h(e,Ie,i),h(e,S,i),t(S,jt),t(S,C),t(C,Rt),t(S,Ot),h(e,Se,i),h(e,A,i),h(e,Pe,i),h(e,N,i),t(N,zt),h(e,Be,i),h(e,P,i),t(P,Lt),t(P,G),t(G,Dt),t(P,Ft),h(e,We,i),h(e,ae,i),t(ae,qt),h(e,Me,i),h(e,U,i),t(U,oe),t(oe,Ht),h(e,je,i),h(e,B,i),t(B,ue),t(ue,$t),t(B,Ct),t(B,re),t(re,Nt)},d(e){e&&a(f),e&&a(me),e&&a(w),e&&a(pe),e&&a(J),e&&a(we),e&&a(T),e&&a(ge),e&&a(_),e&&a(ve),e&&a(I),e&&a(ye),e&&a(X),e&&a(be),e&&a(Y),e&&a(_e),e&&a(v),e&&a(ke),e&&a(Z),e&&a(Ee),e&&a(ee),e&&a(xe),e&&a(k),e&&a(Ae),e&&a(g),e&&a(Te),e&&a(te),e&&a(Ie),e&&a(S),e&&a(Se),e&&a(A),e&&a(Pe),e&&a(N),e&&a(Be),e&&a(P),e&&a(We),e&&a(ae),e&&a(Me),e&&a(U),e&&a(je),e&&a(B)}}}function Na(K){let f,b;const y=[K[0],Pa];let M={$$slots:{default:[Ca]},$$scope:{ctx:K}};for(let m=0;m<y.length;m+=1)M=Gt(M,y[m]);return f=new $a({props:M}),{c(){Ra(f.$$.fragment)},l(m){Oa(f.$$.fragment,m)},m(m,p){za(f,m,p),b=!0},p(m,[p]){const V=p&1?La(y,[p&1&&Ta(m[0]),p&0&&Ta(Pa)]):{};p&2&&(V.$$scope={dirty:p,ctx:m}),f.$set(V)},i(m){b||(Da(f.$$.fragment,m),b=!0)},o(m){Fa(f.$$.fragment,m),b=!1},d(m){qa(f,m)}}}const Pa={title:"Machines Reading Metadata",description:"It's time for the first update from the tech team! The tech team, that's us: intern Bernhard, student of Artificial Intelligence at the University of Linz, and me, Rainer, Senior Researcher at the Data Science & Artificial Intelligence group at the Austrian Institute of Technology.",url:"https://liviaai.at/blog/2022-04-machines-reading-metadata",date:"April 26, 2022"};function Ga(K,f,b){return K.$$set=y=>{b(0,f=Gt(Gt({},f),Ia(y)))},f=Ia(f),[f]}class Va extends Wa{constructor(f){super();Ma(this,f,Ga,Na,ja,{})}}export{Va as default,Pa as metadata};
