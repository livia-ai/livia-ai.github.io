import{S as Pa,i as Wa,s as Ra,C as Jt,w as Na,x as Ca,y as Da,z as Ha,A as Ga,q as La,o as $a,B as Ma,Q as Ba,e as s,t as i,k as d,c as r,a as l,h as o,d as a,m as f,b as B,O as qa,f as Kt,g as m,H as t}from"../../chunks/vendor-f435face.js";import{P as Fa}from"../../chunks/_post-4f5578a2.js";function za(N){let h,y,p,j,u,w,v,Z,c,Le,ee,$e,Me,te,qe,Fe,ae,ze,Ye,C,Je,Ke,Qe,ie,x,Ue,oe,Ve,Xe,O,Ze,et,ve,T,tt,se,at,it,ye,g,ot,D,st,rt,re,lt,nt,le,mt,ht,be,H,dt,_e,b,ft,ne,ut,ct,me,pt,wt,Ee,L,gt,ke,_,A,vt,he,yt,bt,de,_t,Et,kt,fe,xt,At,P,Tt,ue,It,St,xe,$,Gt,Ae,M,Bt,Te,I,jt,ce,Ot,Pt,Ie,W,pe,Wt,Rt,Se,q,Nt,Ge,E,Qt,Be,R,we,Ct,Dt,je,F,Ht,Oe,z,Lt,Pe,Y,$t,We,J,S,ge,Mt,qt,K,Ft;return{c(){h=s("p"),y=i("Following "),p=s("a"),j=i("our partner workshop in May"),u=i(`, we have started to re-assess our
technical processes based on the feedback we received. Two take-away points from the workshop
were:`),w=d(),v=s("ol"),Z=s("li"),c=s("p"),Le=i("When measuring similarity of metadata records, we should focus "),ee=s("strong"),$e=i("only"),Me=i(` on those metadata fields that concern
`),te=s("strong"),qe=i("subject"),Fe=i(" and "),ae=s("strong"),ze=i("theme"),Ye=i(`. This lead us to exclude some fields from our Sentence Embedding workflow (read about
it `),C=s("a"),Je=i("in our earlier blogpost here"),Ke=i(`), in particular those related to
style, collection, and artist.`),Qe=d(),ie=s("li"),x=s("p"),Ue=i("We should consider measuring similarity based "),oe=s("strong"),Ve=i("exclusively"),Xe=i(` on the curator-assigned subject tags, even if
other free-text fields (specifically, title and description) exist. This lead us to experiment with an alternative
machine learning technology called `),O=s("a"),Ze=i("Graph Embedding"),et=i("."),ve=d(),T=s("p"),tt=i(`Another important outcome of our first experiments - which the workshop confirmed - were that the similarity scores we
had computed did not work well `),se=s("strong"),at=i("when combining data from multiple museums"),it=i(". This may be because the vocabularies and curating practices differ; or because the collections are inherently too different in terms of their materials."),ye=d(),g=s("p"),ot=i("This outcome doesn\u2019t exactly make our lives easier - after all, connecting collections is "),D=s("a"),st=i(`one of LiviaAI\u2019s three main
goals`),rt=i(`. But it isn\u2019t entirely unexpected either. In fact, we had expected that metadata alone
would be insufficent to connect across collections. Which is why we proposed the this project in the first place! Our goal is to, first, select artworks with `),re=s("strong"),lt=i("similar metadata from one collection"),nt=i(", and then use "),le=s("strong"),mt=i("their images as input"),ht=i(" to train an AI model to recognize the similarity in the images."),be=d(),H=s("h2"),dt=i("Triplets"),_e=d(),b=s("p"),ft=i("In practice, however, we need more than just pairs of similar images. In order to learn visual similarity, the AI also needs to see examples of images that are "),ne=s("strong"),ut=i("different"),ct=i(" from each other. In AI terminology, a group of three images, where the first two are an example of similar images, and the third one is an example of a different image, is called a "),me=s("strong"),pt=i("triplet"),wt=i("."),Ee=d(),L=s("p"),gt=i("Good triplets will produce an AI model that can measure similarity in a useful way, thus allowing us to identify related images between collections, regardless of their metadata is being recorded in different institutions. To summarize our workflow:"),ke=d(),_=s("ul"),A=s("li"),vt=i("First, we compile lots of triplets, i.e. examples of images that are "),he=s("strong"),yt=i("similar"),bt=i(", and images that are "),de=s("strong"),_t=i("different"),Et=i(" in terms of theme and subject."),kt=d(),fe=s("li"),xt=i("We feed the triplets into the AI, so it learns to \u201Cunderstand\u201D what similarity looks like in the images."),At=d(),P=s("li"),Tt=i("In order to get our examples, we rely on the metadata: because similar subject and themes are described in similar terms "),ue=s("strong"),It=i("within one museum collection"),St=i(`, records with high metadata similarity should provide us with good image training
material.`),xe=d(),$=s("h2"),Gt=i("Comparing Approaches"),Ae=d(),M=s("p"),Bt=i("So far, we\u2019ve built triplets for two different museums (the Belvedere and the Wien Museum), and - as written above - using two different methods: Sentence Embeddings and Graph Embeddings. As a next step, we want to understand more about the real-world applicability of these two approaches."),Te=d(),I=s("p"),jt=i("Both approaches present us with a quantitative measure of similarity that\u2019s\u2026 technically\u2026 accurate: metadata records that include many identical terms, keywords and wording (perhaps with the odd synonym thrown in) "),ce=s("em"),Ot=i("will"),Pt=i(" be reliably identified as similar. But the real world isn\u2019t that simple, of course. Metadata isn\u2019t always rich enough to provide enough material for the algorithm (or even humans!) to judge whether two images are similar enough in terms of subject and themes; and curation may not always be consistent, even within the same institution and collection."),Ie=d(),W=s("p"),pe=s("strong"),Wt=i("That\u2019s why we need your help:"),Rt=i(" Below, you see a random triplet. Based on the metdata, our algorithm has selected images A and B as similar; and image C as different. Does the choice make sense? You can vote using the two buttons below."),Se=d(),q=s("p"),Nt=i("Give us a thumbs-up if images A and B show similar motives or themes, and image C is different from A and B. Give us a thumbs-down if there is little similarity between A and B, or C is too similar to A or B."),Ge=d(),E=s("iframe"),Be=d(),R=s("p"),we=s("strong"),Ct=i("We know that ratings are subjective. That\u2019s no problem!"),Dt=i(` We want to collect as much & as diverse feedback
as possible. Don\u2019t overthink it. Just focus on image motives and themes, and rate according to your intuition.`),je=d(),F=s("p"),Ht=i("After your vote, a new random triplet will load. Keep going as long as you want - the more data we collect, the better. You can also skip a triplet by clicking the \u201CSkip this Triplet\u201D link. Please do this only in cases where you really (really!) cannot decide, or if there\u2019s a problem loading a particular triplet. When in doubt, your gut feeling helps us more than no vote at all."),Oe=d(),z=s("h2"),Lt=i("What Happens with the Data?"),Pe=d(),Y=s("p"),$t=i("[\u2026]"),We=d(),J=s("footer"),S=s("div"),ge=s("h4"),Mt=i("Previous post"),qt=d(),K=s("a"),Ft=i("Community and Data"),this.h()},l(e){h=r(e,"P",{});var n=l(h);y=o(n,"Following "),p=r(n,"A",{href:!0});var Ut=l(p);j=o(Ut,"our partner workshop in May"),Ut.forEach(a),u=o(n,`, we have started to re-assess our
technical processes based on the feedback we received. Two take-away points from the workshop
were:`),n.forEach(a),w=f(e),v=r(e,"OL",{});var Re=l(v);Z=r(Re,"LI",{});var Vt=l(Z);c=r(Vt,"P",{});var k=l(c);Le=o(k,"When measuring similarity of metadata records, we should focus "),ee=r(k,"STRONG",{});var Xt=l(ee);$e=o(Xt,"only"),Xt.forEach(a),Me=o(k,` on those metadata fields that concern
`),te=r(k,"STRONG",{});var Zt=l(te);qe=o(Zt,"subject"),Zt.forEach(a),Fe=o(k," and "),ae=r(k,"STRONG",{});var ea=l(ae);ze=o(ea,"theme"),ea.forEach(a),Ye=o(k,`. This lead us to exclude some fields from our Sentence Embedding workflow (read about
it `),C=r(k,"A",{href:!0});var ta=l(C);Je=o(ta,"in our earlier blogpost here"),ta.forEach(a),Ke=o(k,`), in particular those related to
style, collection, and artist.`),k.forEach(a),Vt.forEach(a),Qe=f(Re),ie=r(Re,"LI",{});var aa=l(ie);x=r(aa,"P",{});var Q=l(x);Ue=o(Q,"We should consider measuring similarity based "),oe=r(Q,"STRONG",{});var ia=l(oe);Ve=o(ia,"exclusively"),ia.forEach(a),Xe=o(Q,` on the curator-assigned subject tags, even if
other free-text fields (specifically, title and description) exist. This lead us to experiment with an alternative
machine learning technology called `),O=r(Q,"A",{href:!0,rel:!0});var oa=l(O);Ze=o(oa,"Graph Embedding"),oa.forEach(a),et=o(Q,"."),Q.forEach(a),aa.forEach(a),Re.forEach(a),ve=f(e),T=r(e,"P",{});var Ne=l(T);tt=o(Ne,`Another important outcome of our first experiments - which the workshop confirmed - were that the similarity scores we
had computed did not work well `),se=r(Ne,"STRONG",{});var sa=l(se);at=o(sa,"when combining data from multiple museums"),sa.forEach(a),it=o(Ne,". This may be because the vocabularies and curating practices differ; or because the collections are inherently too different in terms of their materials."),Ne.forEach(a),ye=f(e),g=r(e,"P",{});var G=l(g);ot=o(G,"This outcome doesn\u2019t exactly make our lives easier - after all, connecting collections is "),D=r(G,"A",{href:!0});var ra=l(D);st=o(ra,`one of LiviaAI\u2019s three main
goals`),ra.forEach(a),rt=o(G,`. But it isn\u2019t entirely unexpected either. In fact, we had expected that metadata alone
would be insufficent to connect across collections. Which is why we proposed the this project in the first place! Our goal is to, first, select artworks with `),re=r(G,"STRONG",{});var la=l(re);lt=o(la,"similar metadata from one collection"),la.forEach(a),nt=o(G,", and then use "),le=r(G,"STRONG",{});var na=l(le);mt=o(na,"their images as input"),na.forEach(a),ht=o(G," to train an AI model to recognize the similarity in the images."),G.forEach(a),be=f(e),H=r(e,"H2",{});var ma=l(H);dt=o(ma,"Triplets"),ma.forEach(a),_e=f(e),b=r(e,"P",{});var U=l(b);ft=o(U,"In practice, however, we need more than just pairs of similar images. In order to learn visual similarity, the AI also needs to see examples of images that are "),ne=r(U,"STRONG",{});var ha=l(ne);ut=o(ha,"different"),ha.forEach(a),ct=o(U," from each other. In AI terminology, a group of three images, where the first two are an example of similar images, and the third one is an example of a different image, is called a "),me=r(U,"STRONG",{});var da=l(me);pt=o(da,"triplet"),da.forEach(a),wt=o(U,"."),U.forEach(a),Ee=f(e),L=r(e,"P",{});var fa=l(L);gt=o(fa,"Good triplets will produce an AI model that can measure similarity in a useful way, thus allowing us to identify related images between collections, regardless of their metadata is being recorded in different institutions. To summarize our workflow:"),fa.forEach(a),ke=f(e),_=r(e,"UL",{});var V=l(_);A=r(V,"LI",{});var X=l(A);vt=o(X,"First, we compile lots of triplets, i.e. examples of images that are "),he=r(X,"STRONG",{});var ua=l(he);yt=o(ua,"similar"),ua.forEach(a),bt=o(X,", and images that are "),de=r(X,"STRONG",{});var ca=l(de);_t=o(ca,"different"),ca.forEach(a),Et=o(X," in terms of theme and subject."),X.forEach(a),kt=f(V),fe=r(V,"LI",{});var pa=l(fe);xt=o(pa,"We feed the triplets into the AI, so it learns to \u201Cunderstand\u201D what similarity looks like in the images."),pa.forEach(a),At=f(V),P=r(V,"LI",{});var Ce=l(P);Tt=o(Ce,"In order to get our examples, we rely on the metadata: because similar subject and themes are described in similar terms "),ue=r(Ce,"STRONG",{});var wa=l(ue);It=o(wa,"within one museum collection"),wa.forEach(a),St=o(Ce,`, records with high metadata similarity should provide us with good image training
material.`),Ce.forEach(a),V.forEach(a),xe=f(e),$=r(e,"H2",{});var ga=l($);Gt=o(ga,"Comparing Approaches"),ga.forEach(a),Ae=f(e),M=r(e,"P",{});var va=l(M);Bt=o(va,"So far, we\u2019ve built triplets for two different museums (the Belvedere and the Wien Museum), and - as written above - using two different methods: Sentence Embeddings and Graph Embeddings. As a next step, we want to understand more about the real-world applicability of these two approaches."),va.forEach(a),Te=f(e),I=r(e,"P",{});var De=l(I);jt=o(De,"Both approaches present us with a quantitative measure of similarity that\u2019s\u2026 technically\u2026 accurate: metadata records that include many identical terms, keywords and wording (perhaps with the odd synonym thrown in) "),ce=r(De,"EM",{});var ya=l(ce);Ot=o(ya,"will"),ya.forEach(a),Pt=o(De," be reliably identified as similar. But the real world isn\u2019t that simple, of course. Metadata isn\u2019t always rich enough to provide enough material for the algorithm (or even humans!) to judge whether two images are similar enough in terms of subject and themes; and curation may not always be consistent, even within the same institution and collection."),De.forEach(a),Ie=f(e),W=r(e,"P",{});var zt=l(W);pe=r(zt,"STRONG",{});var ba=l(pe);Wt=o(ba,"That\u2019s why we need your help:"),ba.forEach(a),Rt=o(zt," Below, you see a random triplet. Based on the metdata, our algorithm has selected images A and B as similar; and image C as different. Does the choice make sense? You can vote using the two buttons below."),zt.forEach(a),Se=f(e),q=r(e,"P",{});var _a=l(q);Nt=o(_a,"Give us a thumbs-up if images A and B show similar motives or themes, and image C is different from A and B. Give us a thumbs-down if there is little similarity between A and B, or C is too similar to A or B."),_a.forEach(a),Ge=f(e),E=r(e,"IFRAME",{src:!0,style:!0});var Oa=l(E);Oa.forEach(a),Be=f(e),R=r(e,"P",{});var Yt=l(R);we=r(Yt,"STRONG",{});var Ea=l(we);Ct=o(Ea,"We know that ratings are subjective. That\u2019s no problem!"),Ea.forEach(a),Dt=o(Yt,` We want to collect as much & as diverse feedback
as possible. Don\u2019t overthink it. Just focus on image motives and themes, and rate according to your intuition.`),Yt.forEach(a),je=f(e),F=r(e,"P",{});var ka=l(F);Ht=o(ka,"After your vote, a new random triplet will load. Keep going as long as you want - the more data we collect, the better. You can also skip a triplet by clicking the \u201CSkip this Triplet\u201D link. Please do this only in cases where you really (really!) cannot decide, or if there\u2019s a problem loading a particular triplet. When in doubt, your gut feeling helps us more than no vote at all."),ka.forEach(a),Oe=f(e),z=r(e,"H2",{});var xa=l(z);Lt=o(xa,"What Happens with the Data?"),xa.forEach(a),Pe=f(e),Y=r(e,"P",{});var Aa=l(Y);$t=o(Aa,"[\u2026]"),Aa.forEach(a),We=f(e),J=r(e,"FOOTER",{});var Ta=l(J);S=r(Ta,"DIV",{class:!0});var He=l(S);ge=r(He,"H4",{});var Ia=l(ge);Mt=o(Ia,"Previous post"),Ia.forEach(a),qt=f(He),K=r(He,"A",{href:!0});var Sa=l(K);Ft=o(Sa,"Community and Data"),Sa.forEach(a),He.forEach(a),Ta.forEach(a),this.h()},h(){B(p,"href","/blog/2022-05-community-data"),B(C,"href","/blog/2022-04-machines-reading-metadata"),B(O,"href","https://towardsdatascience.com/overview-of-deep-learning-on-graph-embeddings-4305c10ad4a4"),B(O,"rel","nofollow"),B(D,"href","/blog/2022-04-hello-world"),qa(E.src,Qt="https://rate-this-triplet.no5.at/ui/")||B(E,"src",Qt),Kt(E,"width","100%"),Kt(E,"height","700px"),Kt(E,"margin","0"),B(K,"href","/blog/2022-05-community-data/"),B(S,"class","previous-post")},m(e,n){m(e,h,n),t(h,y),t(h,p),t(p,j),t(h,u),m(e,w,n),m(e,v,n),t(v,Z),t(Z,c),t(c,Le),t(c,ee),t(ee,$e),t(c,Me),t(c,te),t(te,qe),t(c,Fe),t(c,ae),t(ae,ze),t(c,Ye),t(c,C),t(C,Je),t(c,Ke),t(v,Qe),t(v,ie),t(ie,x),t(x,Ue),t(x,oe),t(oe,Ve),t(x,Xe),t(x,O),t(O,Ze),t(x,et),m(e,ve,n),m(e,T,n),t(T,tt),t(T,se),t(se,at),t(T,it),m(e,ye,n),m(e,g,n),t(g,ot),t(g,D),t(D,st),t(g,rt),t(g,re),t(re,lt),t(g,nt),t(g,le),t(le,mt),t(g,ht),m(e,be,n),m(e,H,n),t(H,dt),m(e,_e,n),m(e,b,n),t(b,ft),t(b,ne),t(ne,ut),t(b,ct),t(b,me),t(me,pt),t(b,wt),m(e,Ee,n),m(e,L,n),t(L,gt),m(e,ke,n),m(e,_,n),t(_,A),t(A,vt),t(A,he),t(he,yt),t(A,bt),t(A,de),t(de,_t),t(A,Et),t(_,kt),t(_,fe),t(fe,xt),t(_,At),t(_,P),t(P,Tt),t(P,ue),t(ue,It),t(P,St),m(e,xe,n),m(e,$,n),t($,Gt),m(e,Ae,n),m(e,M,n),t(M,Bt),m(e,Te,n),m(e,I,n),t(I,jt),t(I,ce),t(ce,Ot),t(I,Pt),m(e,Ie,n),m(e,W,n),t(W,pe),t(pe,Wt),t(W,Rt),m(e,Se,n),m(e,q,n),t(q,Nt),m(e,Ge,n),m(e,E,n),m(e,Be,n),m(e,R,n),t(R,we),t(we,Ct),t(R,Dt),m(e,je,n),m(e,F,n),t(F,Ht),m(e,Oe,n),m(e,z,n),t(z,Lt),m(e,Pe,n),m(e,Y,n),t(Y,$t),m(e,We,n),m(e,J,n),t(J,S),t(S,ge),t(ge,Mt),t(S,qt),t(S,K),t(K,Ft)},d(e){e&&a(h),e&&a(w),e&&a(v),e&&a(ve),e&&a(T),e&&a(ye),e&&a(g),e&&a(be),e&&a(H),e&&a(_e),e&&a(b),e&&a(Ee),e&&a(L),e&&a(ke),e&&a(_),e&&a(xe),e&&a($),e&&a(Ae),e&&a(M),e&&a(Te),e&&a(I),e&&a(Ie),e&&a(W),e&&a(Se),e&&a(q),e&&a(Ge),e&&a(E),e&&a(Be),e&&a(R),e&&a(je),e&&a(F),e&&a(Oe),e&&a(z),e&&a(Pe),e&&a(Y),e&&a(We),e&&a(J)}}}function Ya(N){let h,y;const p=[N[0],ja];let j={$$slots:{default:[za]},$$scope:{ctx:N}};for(let u=0;u<p.length;u+=1)j=Jt(j,p[u]);return h=new Fa({props:j}),{c(){Na(h.$$.fragment)},l(u){Ca(h.$$.fragment,u)},m(u,w){Da(h,u,w),y=!0},p(u,[w]){const v=w&1?Ha(p,[w&1&&Ga(u[0]),w&0&&Ga(ja)]):{};w&2&&(v.$$scope={dirty:w,ctx:u}),h.$set(v)},i(u){y||(La(h.$$.fragment,u),y=!0)},o(u){$a(h.$$.fragment,u),y=!1},d(u){Ma(h,u)}}}const ja={title:"Exploring Similarity: Triplets and How to Choose Them",description:"Help us understand the quality of our data by participating in a small crowdsourcing experiment.",url:"https://liviaai.at/blog/2022-06-exploring-similarity",date:"June 15, 2022"};function Ja(N,h,y){return N.$$set=p=>{y(0,h=Jt(Jt({},h),Ba(p)))},h=Ba(h),[h]}class Ua extends Pa{constructor(h){super();Wa(this,h,Ja,Ya,Ra,{})}}export{Ua as default,ja as metadata};
