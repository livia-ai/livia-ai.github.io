import{S as Pt,i as Rt,s as Lt,e as s,t as r,k as u,c as l,a as n,h as o,d as t,m,b as c,N as Ot,f as Wt,g as h,J as a,K as it}from"../../chunks/vendor-ea2447eb.js";function Bt(jt){let b,de,V,w,we,j,pe,ve,S,ye,be,X,f,ge,P,ke,_e,H,xe,Ee,G,Ae,Me,E,Te,Ie,Y,R,We,Z,g,je,K,Se,Pe,$,p,Re,U,Le,Oe,z,Be,qe,ee,L,Fe,te,O,Ne,ae,B,Ce,re,d,De,A,He,Ge,M,Ke,Ue,T,ze,Je,oe,q,Qe,ie,F,Ve,se,N,Xe,le,k,J,Ye,Ze,I,$e,W,et,tt,ne,_,at,Q,rt,ot,he,y,st;return{c(){b=s("h1"),de=r("Machines Reading Metadata"),V=u(),w=s("p"),we=r("We are now six weeks into our project, and it\u2019s time for an update on the work of our tech team! The tech team, by the way, that\u2019s us: intern and Machine Learning student "),j=s("a"),pe=r("Bernhard"),ve=r(", and me, "),S=s("a"),ye=r("Rainer"),be=r(", Senior Researcher at the Data Science & Artificial Intelligence group at the Austrian Institute of Technology."),X=u(),f=s("p"),ge=r("As we explained in our "),P=s("a"),ke=r("introductory post"),_e=r(`, the main goal of LiviaAI is to find out whether we can teach computers to judge \u201Csimilarity\u201D of artworks. Why?
For two reasons: first, this way museums in the future will be able to `),H=s("strong"),xe=r("more easily connect their collections to those of other museums"),Ee=r(". Computers would handle most of the tedious, time-consuming work of sorting through thousands of artworks, and finding relevant related material in other museums, thus saving museum curators many hours of their valuable time. Second, we believe that these new connections will enable "),G=s("strong"),Ae=r("new forms of online museum experiences that cross institutional boundaries"),Me=r(". Interested in the "),E=s("a"),Te=r("Wiener Moderne"),Ie=r("? Then why stay within the confines of just one or two famous museums, when other, smaller, museums might house some lesser known treasures, too?"),Y=u(),R=s("h2"),We=r("Similar or Not?"),Z=u(),g=s("p"),je=r("The big question is, of course: what "),K=s("em"),Se=r("does"),Pe=r(" make two artworks similar? And how do we even get enough of the right data to teach a machine to mimic our understanding of similarity? Previous approaches have been looking, for example, at similar colours. Or they used artificial intelligence to detect objects in the artworks. But would you necessarily consider two artworks similar just because they both use yellow as their primary color? Or because they both depict flowers?"),$=u(),p=s("p"),Re=r("In LiviaAI, we want to take a different approach. Instead of coming up with our own metrics for similarity, or relying on technical properties like color, we want to leverage the existing knowledge that museum curators have accumulated in their collections over the years. Before teaching computers how to recognize similarity in the "),U=s("em"),Le=r("images"),Oe=r(", we first want to understand how curators have been "),z=s("em"),Be=r("describing"),qe=r(" their artworks, and search for similarities there."),ee=u(),L=s("h2"),Fe=r("Understanding our Partner Collections"),te=u(),O=s("p"),Ne=r("The first step in our journey is therefore to get a better understanding of our partner collections. What\u2019s actually inside them? What information have curators added to the items as metadata over the years? And how can we translate all this existing knowledge - and hard work - into data that computers can use to learn about similarity?"),ae=u(),B=s("p"),Ce=r("[\u2026perhaps two examples of artworks + metadata - maybe the Lusthaus from two partners?\u2026]"),re=u(),d=s("p"),De=r("Our three partners, the "),A=s("a"),He=r("Belvedere Museum Wien"),Ge=r(", the "),M=s("a"),Ke=r("Museum f\xFCr angewandte Kunst"),Ue=r(" and the "),T=s("a"),ze=r("Wien Museum"),Je=r(" have given us access to collection metadata that pertains to all artworks displayed in their public online collections. That\u2019s more than 300.000 objects in total! The metadata includes a mix of information such as: title and description text, subjects keywords and classification terms, artist names, summary data on provenance, and, where applicable, information about material, production, places and time periods relevant to the artwork."),oe=u(),q=s("p"),Qe=r("Each partner manages their metadata in slightly different ways. Therefore, figuring out which metadata records might lead us to artworks that are similar - according to what the curators tell us about them - is not quite straightforward yet."),ie=u(),F=s("h2"),Ve=r("Reading Metadata - from a Distance"),se=u(),N=s("p"),Xe=r("[\u2026unfinished\u2026 material for re-use \u2026]"),le=u(),k=s("ul"),J=s("li"),Ye=r(`First, we will collect examples of images that are \u201Csimilar\u201D in some way. We will also collect counter-examples, so that the
computer can learn what \u201Cdifferent\u201D images look like. (More on the ways in which images might resemble or differ later.)`),Ze=u(),I=s("li"),$e=r(`Second, we will use the examples to train an AI algorithm, so that it learns how \u201Csimilarity\u201D manifests itself in the image. Or,
in other words: so that it learns what to look for in similar vs. different images. (This process is called
`),W=s("a"),et=r("representation or feature learning"),tt=r(")."),ne=u(),_=s("p"),at=r("The bad news: we need "),Q=s("strong"),rt=r("a lot"),ot=r(" of examples to train the AI. We expect that we\u2019ll need at least 10.000 examples as the absolute minimum. And that\u2019s a much bigger training set than we could ever assemble by hand."),he=u(),y=s("iframe"),this.h()},l(e){b=l(e,"H1",{});var i=n(b);de=o(i,"Machines Reading Metadata"),i.forEach(t),V=m(e),w=l(e,"P",{});var C=n(w);we=o(C,"We are now six weeks into our project, and it\u2019s time for an update on the work of our tech team! The tech team, by the way, that\u2019s us: intern and Machine Learning student "),j=l(C,"A",{href:!0});var lt=n(j);pe=o(lt,"Bernhard"),lt.forEach(t),ve=o(C,", and me, "),S=l(C,"A",{href:!0});var nt=n(S);ye=o(nt,"Rainer"),nt.forEach(t),be=o(C,", Senior Researcher at the Data Science & Artificial Intelligence group at the Austrian Institute of Technology."),C.forEach(t),X=m(e),f=l(e,"P",{});var v=n(f);ge=o(v,"As we explained in our "),P=l(v,"A",{href:!0});var ht=n(P);ke=o(ht,"introductory post"),ht.forEach(t),_e=o(v,`, the main goal of LiviaAI is to find out whether we can teach computers to judge \u201Csimilarity\u201D of artworks. Why?
For two reasons: first, this way museums in the future will be able to `),H=l(v,"STRONG",{});var ut=n(H);xe=o(ut,"more easily connect their collections to those of other museums"),ut.forEach(t),Ee=o(v,". Computers would handle most of the tedious, time-consuming work of sorting through thousands of artworks, and finding relevant related material in other museums, thus saving museum curators many hours of their valuable time. Second, we believe that these new connections will enable "),G=l(v,"STRONG",{});var mt=n(G);Ae=o(mt,"new forms of online museum experiences that cross institutional boundaries"),mt.forEach(t),Me=o(v,". Interested in the "),E=l(v,"A",{href:!0,rel:!0});var ft=n(E);Te=o(ft,"Wiener Moderne"),ft.forEach(t),Ie=o(v,"? Then why stay within the confines of just one or two famous museums, when other, smaller, museums might house some lesser known treasures, too?"),v.forEach(t),Y=m(e),R=l(e,"H2",{});var ct=n(R);We=o(ct,"Similar or Not?"),ct.forEach(t),Z=m(e),g=l(e,"P",{});var ue=n(g);je=o(ue,"The big question is, of course: what "),K=l(ue,"EM",{});var dt=n(K);Se=o(dt,"does"),dt.forEach(t),Pe=o(ue," make two artworks similar? And how do we even get enough of the right data to teach a machine to mimic our understanding of similarity? Previous approaches have been looking, for example, at similar colours. Or they used artificial intelligence to detect objects in the artworks. But would you necessarily consider two artworks similar just because they both use yellow as their primary color? Or because they both depict flowers?"),ue.forEach(t),$=m(e),p=l(e,"P",{});var D=n(p);Re=o(D,"In LiviaAI, we want to take a different approach. Instead of coming up with our own metrics for similarity, or relying on technical properties like color, we want to leverage the existing knowledge that museum curators have accumulated in their collections over the years. Before teaching computers how to recognize similarity in the "),U=l(D,"EM",{});var wt=n(U);Le=o(wt,"images"),wt.forEach(t),Oe=o(D,", we first want to understand how curators have been "),z=l(D,"EM",{});var pt=n(z);Be=o(pt,"describing"),pt.forEach(t),qe=o(D," their artworks, and search for similarities there."),D.forEach(t),ee=m(e),L=l(e,"H2",{});var vt=n(L);Fe=o(vt,"Understanding our Partner Collections"),vt.forEach(t),te=m(e),O=l(e,"P",{});var yt=n(O);Ne=o(yt,"The first step in our journey is therefore to get a better understanding of our partner collections. What\u2019s actually inside them? What information have curators added to the items as metadata over the years? And how can we translate all this existing knowledge - and hard work - into data that computers can use to learn about similarity?"),yt.forEach(t),ae=m(e),B=l(e,"P",{});var bt=n(B);Ce=o(bt,"[\u2026perhaps two examples of artworks + metadata - maybe the Lusthaus from two partners?\u2026]"),bt.forEach(t),re=m(e),d=l(e,"P",{});var x=n(d);De=o(x,"Our three partners, the "),A=l(x,"A",{href:!0,rel:!0});var gt=n(A);He=o(gt,"Belvedere Museum Wien"),gt.forEach(t),Ge=o(x,", the "),M=l(x,"A",{href:!0,rel:!0});var kt=n(M);Ke=o(kt,"Museum f\xFCr angewandte Kunst"),kt.forEach(t),Ue=o(x," and the "),T=l(x,"A",{href:!0,rel:!0});var _t=n(T);ze=o(_t,"Wien Museum"),_t.forEach(t),Je=o(x," have given us access to collection metadata that pertains to all artworks displayed in their public online collections. That\u2019s more than 300.000 objects in total! The metadata includes a mix of information such as: title and description text, subjects keywords and classification terms, artist names, summary data on provenance, and, where applicable, information about material, production, places and time periods relevant to the artwork."),x.forEach(t),oe=m(e),q=l(e,"P",{});var xt=n(q);Qe=o(xt,"Each partner manages their metadata in slightly different ways. Therefore, figuring out which metadata records might lead us to artworks that are similar - according to what the curators tell us about them - is not quite straightforward yet."),xt.forEach(t),ie=m(e),F=l(e,"H2",{});var Et=n(F);Ve=o(Et,"Reading Metadata - from a Distance"),Et.forEach(t),se=m(e),N=l(e,"P",{});var At=n(N);Xe=o(At,"[\u2026unfinished\u2026 material for re-use \u2026]"),At.forEach(t),le=m(e),k=l(e,"UL",{});var me=n(k);J=l(me,"LI",{});var Mt=n(J);Ye=o(Mt,`First, we will collect examples of images that are \u201Csimilar\u201D in some way. We will also collect counter-examples, so that the
computer can learn what \u201Cdifferent\u201D images look like. (More on the ways in which images might resemble or differ later.)`),Mt.forEach(t),Ze=m(me),I=l(me,"LI",{});var fe=n(I);$e=o(fe,`Second, we will use the examples to train an AI algorithm, so that it learns how \u201Csimilarity\u201D manifests itself in the image. Or,
in other words: so that it learns what to look for in similar vs. different images. (This process is called
`),W=l(fe,"A",{href:!0,rel:!0});var Tt=n(W);et=o(Tt,"representation or feature learning"),Tt.forEach(t),tt=o(fe,")."),fe.forEach(t),me.forEach(t),ne=m(e),_=l(e,"P",{});var ce=n(_);at=o(ce,"The bad news: we need "),Q=l(ce,"STRONG",{});var It=n(Q);rt=o(It,"a lot"),It.forEach(t),ot=o(ce," of examples to train the AI. We expect that we\u2019ll need at least 10.000 examples as the absolute minimum. And that\u2019s a much bigger training set than we could ever assemble by hand."),ce.forEach(t),he=m(e),y=l(e,"IFRAME",{src:!0,style:!0});var St=n(y);St.forEach(t),this.h()},h(){c(j,"href","#"),c(S,"href","#"),c(P,"href","#"),c(E,"href","https://en.wikipedia.org/wiki/Wiener_Moderne"),c(E,"rel","nofollow"),c(A,"href","https://www.belvedere.at/"),c(A,"rel","nofollow"),c(M,"href","https://www.mak.at"),c(M,"rel","nofollow"),c(T,"href","https://www.wienmuseum.at/"),c(T,"rel","nofollow"),c(W,"href","https://en.wikipedia.org/wiki/Feature_learning"),c(W,"rel","nofollow"),Ot(y.src,st="/embeds/blog/2022-04/embeddings-example.html")||c(y,"src",st),Wt(y,"width","800px"),Wt(y,"height","600px")},m(e,i){h(e,b,i),a(b,de),h(e,V,i),h(e,w,i),a(w,we),a(w,j),a(j,pe),a(w,ve),a(w,S),a(S,ye),a(w,be),h(e,X,i),h(e,f,i),a(f,ge),a(f,P),a(P,ke),a(f,_e),a(f,H),a(H,xe),a(f,Ee),a(f,G),a(G,Ae),a(f,Me),a(f,E),a(E,Te),a(f,Ie),h(e,Y,i),h(e,R,i),a(R,We),h(e,Z,i),h(e,g,i),a(g,je),a(g,K),a(K,Se),a(g,Pe),h(e,$,i),h(e,p,i),a(p,Re),a(p,U),a(U,Le),a(p,Oe),a(p,z),a(z,Be),a(p,qe),h(e,ee,i),h(e,L,i),a(L,Fe),h(e,te,i),h(e,O,i),a(O,Ne),h(e,ae,i),h(e,B,i),a(B,Ce),h(e,re,i),h(e,d,i),a(d,De),a(d,A),a(A,He),a(d,Ge),a(d,M),a(M,Ke),a(d,Ue),a(d,T),a(T,ze),a(d,Je),h(e,oe,i),h(e,q,i),a(q,Qe),h(e,ie,i),h(e,F,i),a(F,Ve),h(e,se,i),h(e,N,i),a(N,Xe),h(e,le,i),h(e,k,i),a(k,J),a(J,Ye),a(k,Ze),a(k,I),a(I,$e),a(I,W),a(W,et),a(I,tt),h(e,ne,i),h(e,_,i),a(_,at),a(_,Q),a(Q,rt),a(_,ot),h(e,he,i),h(e,y,i)},p:it,i:it,o:it,d(e){e&&t(b),e&&t(V),e&&t(w),e&&t(X),e&&t(f),e&&t(Y),e&&t(R),e&&t(Z),e&&t(g),e&&t($),e&&t(p),e&&t(ee),e&&t(L),e&&t(te),e&&t(O),e&&t(ae),e&&t(B),e&&t(re),e&&t(d),e&&t(oe),e&&t(q),e&&t(ie),e&&t(F),e&&t(se),e&&t(N),e&&t(le),e&&t(k),e&&t(ne),e&&t(_),e&&t(he),e&&t(y)}}}class Ft extends Pt{constructor(b){super();Rt(this,b,null,Bt,Lt,{})}}export{Ft as default};
