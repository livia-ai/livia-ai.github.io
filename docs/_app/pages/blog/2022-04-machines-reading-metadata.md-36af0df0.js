import{S as Vt,i as Xt,s as Zt,C as _t,w as ea,x as ta,y as aa,z as ra,A as oa,q as ia,o as sa,B as na,O as Yt,e as s,t as r,k as m,c as n,a as l,h as o,d as t,m as d,b as u,N as la,f as Jt,g as h,J as a}from"../../chunks/vendor-b56f9969.js";import{P as ha}from"../../chunks/_post-749b8f63.js";function ca(C){let c,y,b,p,f,g,F,Ae,N,Ie,Te,se,w,Me,D,Se,Pe,ee,je,We,te,Be,Oe,P,Le,Re,ne,G,ze,le,I,qe,ae,$e,Ce,he,_,Fe,re,Ne,De,oe,Ge,He,ce,T,Ke,j,Ue,Ye,ue,H,Je,fe,K,Qe,me,U,Ve,de,v,Xe,W,Ze,et,B,tt,at,O,rt,ot,pe,Y,it,we,J,st,ge,k,nt,L,lt,ht,ie,ct,ut,ve,E,ft,R,mt,dt,z,pt,wt,ye,M,gt,q,vt,yt,be,A,kt,_e,$,bt;return{c(){c=s("h1"),y=r("Machines Reading Metadata"),b=m(),p=s("p"),f=r("It\u2019s time for an update on the work of our tech team! The tech team, that\u2019s us: intern "),g=s("a"),F=r("Bernhard"),Ae=r(", student of Artificial Intelligence at the University of Linz, and me, "),N=s("a"),Ie=r("Rainer"),Te=r(", Senior Researcher at the Data Science & Artificial Intelligence group at the Austrian Institute of Technology."),se=m(),w=s("p"),Me=r("As we explained in our "),D=s("a"),Se=r("introductory post"),Pe=r(", the main goal of LiviaAI is to find out whether we can teach computers to recognize \u201Csimilarity\u201D of artworks. Why? For two reasons: first, because this will allow museums in the future to "),ee=s("strong"),je=r("connect their collections to those of other museums more easily"),We=r(". Computers can handle most of the tedious, time-consuming work of sorting through thousands of artworks, and finding relevant related material in other museums, thus saving museum curators many hours of their valuable time. Second, we believe that these new connections will enable "),te=s("strong"),Be=r("new forms of online museum experiences that cross institutional boundaries"),Oe=r(". Interested in the "),P=s("a"),Le=r("Wiener Moderne"),Re=r("? Then why stay within the confines of just one or two famous museums, when other, smaller, museums might house some lesser known treasures, too?"),ne=m(),G=s("h2"),ze=r("Similar or Not?"),le=m(),I=s("p"),qe=r("The big question is, of course: what "),ae=s("em"),$e=r("does"),Ce=r(" make two artworks similar? And how do we even get enough of the right data to teach a machine to mimic our understanding of similarity? Previous approaches have been looking, for example, at similar colours. Or they used artificial intelligence to detect common objects in the artworks. But would you necessarily consider two artworks similar just because they both use yellow as their primary color? Or because they both depict flowers?"),he=m(),_=s("p"),Fe=r("In LiviaAI, we want to take a different approach. Instead of coming up with our own metrics for similarity, or relying on technical properties like color, we want to leverage the existing knowledge that museum curators have accumulated in their collections over many years. In order to teach computers how to recognize similar "),re=s("em"),Ne=r("images"),De=r(", we first want to understand how curators have been describing them - and search for similarity in these "),oe=s("em"),Ge=r("descriptions"),He=r("."),ce=m(),T=s("p"),Ke=r("Once we have identified artworks that are described in similar terms, we will use their images to train the algorithm. This approach is also called "),j=s("a"),Ue=r("representation or feature learning"),Ye=r(", because the algorithm learns - by itself - how the representation of similiarity manifests itself in the images."),ue=m(),H=s("h2"),Je=r("Understanding Our Partner Collections"),fe=m(),K=s("p"),Qe=r("The first step in our journey is therefore to gain an understanding of our partner collections. What\u2019s actually inside them? What information have curators added to the items as metadata? And how can we translate all this existing knowledge (and years of work!) into data that can teach computers to recognize similar art?"),me=m(),U=s("p"),Ve=r("[\u2026perhaps two examples of artworks + metadata - maybe the Lusthaus from two partners?\u2026]"),de=m(),v=s("p"),Xe=r("Our three partners, the "),W=s("a"),Ze=r("Belvedere Museum Wien"),et=r(", the "),B=s("a"),tt=r("Museum f\xFCr angewandte Kunst"),at=r(" and the "),O=s("a"),rt=r("Wien Museum"),ot=r(" have given us access to metadata for all artworks they display in their public online collections. In total, that\u2019s more than 300.000 objects! The metadata includes a mix of information such as: title and description text, subjects keywords and classification terms, artist names, summary information on provenance or exhibition history, information about material, production, places and time periods relevant to the artwork, etc."),pe=m(),Y=s("p"),it=r("Each partner uses different documentation practices, and has different priorities when describing their artworks. Properties that are recorded in the MAK collection, for example, may not be relevant in the Belvedere collection. Therefore, figuring out which artworks might be similar - according to what the curators tell us about them in the metadata - is not quite straightforward."),we=m(),J=s("h2"),st=r("Reading Metadata - From a Distance"),ge=m(),k=s("p"),nt=r("Luckily, artificial intelligence can help us here, too. In LiviaAI, we use a technique called "),L=s("a"),lt=r("Sentence Embedding"),ht=r(" to get a better picture of the structure, themes and topics of our collections. Sentence embedding transforms written text - such as a sentence or a paragraph - to a "),ie=s("em"),ct=r("vector"),ut=r(", a numeric representation we can treat as a point in space. There are different algorithms for creating embeddings. Some start on a blank canvas, meaning there are no pre-trained assumptions about the data. You hand them your dataset (say, the entire text of a novel), and the algorithm will organize the sentences into the vector space so that sentences that use (or are surrounded by) similar vocabulary are closer together."),ve=m(),E=s("p"),ft=r("Other algorithms come with pre-loaded information. In our case, we use "),R=s("a"),mt=r("BERT"),dt=r(" an AI language model developed in 2018 by Google researchers, which was subsequently adapted for the purpose of high-performance "),z=s("a"),pt=r("sentence embedding"),wt=r(". The advantage of using a pre-trained language model is that it already carries a large amount of latent knowledge about how words and concepts relate in a language. In simplified terms, the model already encodes the information that a \u201Cpair of boots\u201D is closer in meaning to a \u201Cpair of sneakers\u201D than  \u201Cpair of pants\u201D. It would place \u201Cdrawing\u201D at a certain distance from \u201Cphotograph\u201D. But \u201Cdrawing of a person\u201D might be closer \u201Cphotograph of a person\u201D than \u201Cdrawing\u201D and \u201Cphotograph\u201D alone."),ye=m(),M=s("p"),gt=r("The idea of using sentence embeddings to organize our metdata first, and then pick training images from the results, was what triggered us to propose the LiviaAI project in the first place. It was tested successfully "),q=s("a"),vt=r("in a different context, and with a slightly different approach by AIT colleague Alexander Schindler and colleagues"),yt=r(". Which means we\u2026 sort of\u2026 know it will work. But it all depends significantly on the data - it\u2019s level of detail, how much information each individual record contains, whether curators have used free text vs keywords, whether individual records differ sufficiently or whether information is often sparse and formulaic or repetitive, etc."),be=m(),A=s("iframe"),_e=m(),$=s("span"),bt=r("Fig. 2. Sentence embeddings for 6,200 metadata records from the online collection of the Wien Museum, projected to 3 dimensions. Color represents a distinct combination of classifications assigned by the curators."),this.h()},l(e){c=n(e,"H1",{});var i=l(c);y=o(i,"Machines Reading Metadata"),i.forEach(t),b=d(e),p=n(e,"P",{});var Q=l(p);f=o(Q,"It\u2019s time for an update on the work of our tech team! The tech team, that\u2019s us: intern "),g=n(Q,"A",{href:!0});var Et=l(g);F=o(Et,"Bernhard"),Et.forEach(t),Ae=o(Q,", student of Artificial Intelligence at the University of Linz, and me, "),N=n(Q,"A",{href:!0});var xt=l(N);Ie=o(xt,"Rainer"),xt.forEach(t),Te=o(Q,", Senior Researcher at the Data Science & Artificial Intelligence group at the Austrian Institute of Technology."),Q.forEach(t),se=d(e),w=n(e,"P",{});var x=l(w);Me=o(x,"As we explained in our "),D=n(x,"A",{href:!0});var At=l(D);Se=o(At,"introductory post"),At.forEach(t),Pe=o(x,", the main goal of LiviaAI is to find out whether we can teach computers to recognize \u201Csimilarity\u201D of artworks. Why? For two reasons: first, because this will allow museums in the future to "),ee=n(x,"STRONG",{});var It=l(ee);je=o(It,"connect their collections to those of other museums more easily"),It.forEach(t),We=o(x,". Computers can handle most of the tedious, time-consuming work of sorting through thousands of artworks, and finding relevant related material in other museums, thus saving museum curators many hours of their valuable time. Second, we believe that these new connections will enable "),te=n(x,"STRONG",{});var Tt=l(te);Be=o(Tt,"new forms of online museum experiences that cross institutional boundaries"),Tt.forEach(t),Oe=o(x,". Interested in the "),P=n(x,"A",{href:!0,rel:!0});var Mt=l(P);Le=o(Mt,"Wiener Moderne"),Mt.forEach(t),Re=o(x,"? Then why stay within the confines of just one or two famous museums, when other, smaller, museums might house some lesser known treasures, too?"),x.forEach(t),ne=d(e),G=n(e,"H2",{});var St=l(G);ze=o(St,"Similar or Not?"),St.forEach(t),le=d(e),I=n(e,"P",{});var ke=l(I);qe=o(ke,"The big question is, of course: what "),ae=n(ke,"EM",{});var Pt=l(ae);$e=o(Pt,"does"),Pt.forEach(t),Ce=o(ke," make two artworks similar? And how do we even get enough of the right data to teach a machine to mimic our understanding of similarity? Previous approaches have been looking, for example, at similar colours. Or they used artificial intelligence to detect common objects in the artworks. But would you necessarily consider two artworks similar just because they both use yellow as their primary color? Or because they both depict flowers?"),ke.forEach(t),he=d(e),_=n(e,"P",{});var V=l(_);Fe=o(V,"In LiviaAI, we want to take a different approach. Instead of coming up with our own metrics for similarity, or relying on technical properties like color, we want to leverage the existing knowledge that museum curators have accumulated in their collections over many years. In order to teach computers how to recognize similar "),re=n(V,"EM",{});var jt=l(re);Ne=o(jt,"images"),jt.forEach(t),De=o(V,", we first want to understand how curators have been describing them - and search for similarity in these "),oe=n(V,"EM",{});var Wt=l(oe);Ge=o(Wt,"descriptions"),Wt.forEach(t),He=o(V,"."),V.forEach(t),ce=d(e),T=n(e,"P",{});var Ee=l(T);Ke=o(Ee,"Once we have identified artworks that are described in similar terms, we will use their images to train the algorithm. This approach is also called "),j=n(Ee,"A",{href:!0,rel:!0});var Bt=l(j);Ue=o(Bt,"representation or feature learning"),Bt.forEach(t),Ye=o(Ee,", because the algorithm learns - by itself - how the representation of similiarity manifests itself in the images."),Ee.forEach(t),ue=d(e),H=n(e,"H2",{});var Ot=l(H);Je=o(Ot,"Understanding Our Partner Collections"),Ot.forEach(t),fe=d(e),K=n(e,"P",{});var Lt=l(K);Qe=o(Lt,"The first step in our journey is therefore to gain an understanding of our partner collections. What\u2019s actually inside them? What information have curators added to the items as metadata? And how can we translate all this existing knowledge (and years of work!) into data that can teach computers to recognize similar art?"),Lt.forEach(t),me=d(e),U=n(e,"P",{});var Rt=l(U);Ve=o(Rt,"[\u2026perhaps two examples of artworks + metadata - maybe the Lusthaus from two partners?\u2026]"),Rt.forEach(t),de=d(e),v=n(e,"P",{});var S=l(v);Xe=o(S,"Our three partners, the "),W=n(S,"A",{href:!0,rel:!0});var zt=l(W);Ze=o(zt,"Belvedere Museum Wien"),zt.forEach(t),et=o(S,", the "),B=n(S,"A",{href:!0,rel:!0});var qt=l(B);tt=o(qt,"Museum f\xFCr angewandte Kunst"),qt.forEach(t),at=o(S," and the "),O=n(S,"A",{href:!0,rel:!0});var $t=l(O);rt=o($t,"Wien Museum"),$t.forEach(t),ot=o(S," have given us access to metadata for all artworks they display in their public online collections. In total, that\u2019s more than 300.000 objects! The metadata includes a mix of information such as: title and description text, subjects keywords and classification terms, artist names, summary information on provenance or exhibition history, information about material, production, places and time periods relevant to the artwork, etc."),S.forEach(t),pe=d(e),Y=n(e,"P",{});var Ct=l(Y);it=o(Ct,"Each partner uses different documentation practices, and has different priorities when describing their artworks. Properties that are recorded in the MAK collection, for example, may not be relevant in the Belvedere collection. Therefore, figuring out which artworks might be similar - according to what the curators tell us about them in the metadata - is not quite straightforward."),Ct.forEach(t),we=d(e),J=n(e,"H2",{});var Ft=l(J);st=o(Ft,"Reading Metadata - From a Distance"),Ft.forEach(t),ge=d(e),k=n(e,"P",{});var X=l(k);nt=o(X,"Luckily, artificial intelligence can help us here, too. In LiviaAI, we use a technique called "),L=n(X,"A",{href:!0,rel:!0});var Nt=l(L);lt=o(Nt,"Sentence Embedding"),Nt.forEach(t),ht=o(X," to get a better picture of the structure, themes and topics of our collections. Sentence embedding transforms written text - such as a sentence or a paragraph - to a "),ie=n(X,"EM",{});var Dt=l(ie);ct=o(Dt,"vector"),Dt.forEach(t),ut=o(X,", a numeric representation we can treat as a point in space. There are different algorithms for creating embeddings. Some start on a blank canvas, meaning there are no pre-trained assumptions about the data. You hand them your dataset (say, the entire text of a novel), and the algorithm will organize the sentences into the vector space so that sentences that use (or are surrounded by) similar vocabulary are closer together."),X.forEach(t),ve=d(e),E=n(e,"P",{});var Z=l(E);ft=o(Z,"Other algorithms come with pre-loaded information. In our case, we use "),R=n(Z,"A",{href:!0,rel:!0});var Gt=l(R);mt=o(Gt,"BERT"),Gt.forEach(t),dt=o(Z," an AI language model developed in 2018 by Google researchers, which was subsequently adapted for the purpose of high-performance "),z=n(Z,"A",{href:!0,rel:!0});var Ht=l(z);pt=o(Ht,"sentence embedding"),Ht.forEach(t),wt=o(Z,". The advantage of using a pre-trained language model is that it already carries a large amount of latent knowledge about how words and concepts relate in a language. In simplified terms, the model already encodes the information that a \u201Cpair of boots\u201D is closer in meaning to a \u201Cpair of sneakers\u201D than  \u201Cpair of pants\u201D. It would place \u201Cdrawing\u201D at a certain distance from \u201Cphotograph\u201D. But \u201Cdrawing of a person\u201D might be closer \u201Cphotograph of a person\u201D than \u201Cdrawing\u201D and \u201Cphotograph\u201D alone."),Z.forEach(t),ye=d(e),M=n(e,"P",{});var xe=l(M);gt=o(xe,"The idea of using sentence embeddings to organize our metdata first, and then pick training images from the results, was what triggered us to propose the LiviaAI project in the first place. It was tested successfully "),q=n(xe,"A",{href:!0,rel:!0});var Kt=l(q);vt=o(Kt,"in a different context, and with a slightly different approach by AIT colleague Alexander Schindler and colleagues"),Kt.forEach(t),yt=o(xe,". Which means we\u2026 sort of\u2026 know it will work. But it all depends significantly on the data - it\u2019s level of detail, how much information each individual record contains, whether curators have used free text vs keywords, whether individual records differ sufficiently or whether information is often sparse and formulaic or repetitive, etc."),xe.forEach(t),be=d(e),A=n(e,"IFRAME",{src:!0,style:!0});var Qt=l(A);Qt.forEach(t),_e=d(e),$=n(e,"SPAN",{class:!0});var Ut=l($);bt=o(Ut,"Fig. 2. Sentence embeddings for 6,200 metadata records from the online collection of the Wien Museum, projected to 3 dimensions. Color represents a distinct combination of classifications assigned by the curators."),Ut.forEach(t),this.h()},h(){u(g,"href","#"),u(N,"href","#"),u(D,"href","#"),u(P,"href","https://en.wikipedia.org/wiki/Wiener_Moderne"),u(P,"rel","nofollow"),u(j,"href","https://en.wikipedia.org/wiki/Feature_learning"),u(j,"rel","nofollow"),u(W,"href","https://www.belvedere.at/"),u(W,"rel","nofollow"),u(B,"href","https://www.mak.at"),u(B,"rel","nofollow"),u(O,"href","https://www.wienmuseum.at/"),u(O,"rel","nofollow"),u(L,"href","https://en.wikipedia.org/wiki/Sentence_embedding"),u(L,"rel","nofollow"),u(R,"href","https://en.wikipedia.org/wiki/BERT_(language_model)"),u(R,"rel","nofollow"),u(z,"href","https://arxiv.org/abs/1908.10084"),u(z,"rel","nofollow"),u(q,"href","https://arxiv.org/pdf/2003.12265.pdf"),u(q,"rel","nofollow"),la(A.src,kt="/embeds/blog/2022-04/embeddings-example.html")||u(A,"src",kt),Jt(A,"width","800px"),Jt(A,"height","600px"),u($,"class","image-caption")},m(e,i){h(e,c,i),a(c,y),h(e,b,i),h(e,p,i),a(p,f),a(p,g),a(g,F),a(p,Ae),a(p,N),a(N,Ie),a(p,Te),h(e,se,i),h(e,w,i),a(w,Me),a(w,D),a(D,Se),a(w,Pe),a(w,ee),a(ee,je),a(w,We),a(w,te),a(te,Be),a(w,Oe),a(w,P),a(P,Le),a(w,Re),h(e,ne,i),h(e,G,i),a(G,ze),h(e,le,i),h(e,I,i),a(I,qe),a(I,ae),a(ae,$e),a(I,Ce),h(e,he,i),h(e,_,i),a(_,Fe),a(_,re),a(re,Ne),a(_,De),a(_,oe),a(oe,Ge),a(_,He),h(e,ce,i),h(e,T,i),a(T,Ke),a(T,j),a(j,Ue),a(T,Ye),h(e,ue,i),h(e,H,i),a(H,Je),h(e,fe,i),h(e,K,i),a(K,Qe),h(e,me,i),h(e,U,i),a(U,Ve),h(e,de,i),h(e,v,i),a(v,Xe),a(v,W),a(W,Ze),a(v,et),a(v,B),a(B,tt),a(v,at),a(v,O),a(O,rt),a(v,ot),h(e,pe,i),h(e,Y,i),a(Y,it),h(e,we,i),h(e,J,i),a(J,st),h(e,ge,i),h(e,k,i),a(k,nt),a(k,L),a(L,lt),a(k,ht),a(k,ie),a(ie,ct),a(k,ut),h(e,ve,i),h(e,E,i),a(E,ft),a(E,R),a(R,mt),a(E,dt),a(E,z),a(z,pt),a(E,wt),h(e,ye,i),h(e,M,i),a(M,gt),a(M,q),a(q,vt),a(M,yt),h(e,be,i),h(e,A,i),h(e,_e,i),h(e,$,i),a($,bt)},d(e){e&&t(c),e&&t(b),e&&t(p),e&&t(se),e&&t(w),e&&t(ne),e&&t(G),e&&t(le),e&&t(I),e&&t(he),e&&t(_),e&&t(ce),e&&t(T),e&&t(ue),e&&t(H),e&&t(fe),e&&t(K),e&&t(me),e&&t(U),e&&t(de),e&&t(v),e&&t(pe),e&&t(Y),e&&t(we),e&&t(J),e&&t(ge),e&&t(k),e&&t(ve),e&&t(E),e&&t(ye),e&&t(M),e&&t(be),e&&t(A),e&&t(_e),e&&t($)}}}function ua(C){let c,y;const b=[C[0]];let p={$$slots:{default:[ca]},$$scope:{ctx:C}};for(let f=0;f<b.length;f+=1)p=_t(p,b[f]);return c=new ha({props:p}),{c(){ea(c.$$.fragment)},l(f){ta(c.$$.fragment,f)},m(f,g){aa(c,f,g),y=!0},p(f,[g]){const F=g&1?ra(b,[oa(f[0])]):{};g&2&&(F.$$scope={dirty:g,ctx:f}),c.$set(F)},i(f){y||(ia(c.$$.fragment,f),y=!0)},o(f){sa(c.$$.fragment,f),y=!1},d(f){na(c,f)}}}function fa(C,c,y){return C.$$set=b=>{y(0,c=_t(_t({},c),Yt(b)))},c=Yt(c),[c]}class pa extends Vt{constructor(c){super();Xt(this,c,fa,ua,Zt,{})}}export{pa as default};
